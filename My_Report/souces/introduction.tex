\chapter*{Introduction} 
\addcontentsline{toc}{chapter}{Introduction}
\markboth{}{INTRODUCTION}


The few past years have seen a major change in computing systems, as data volumes is growing more and more. In both commercial and scientific fields, new data sources and instruments are producing rapidly increasing amount of information. Processing such amount of data in order to infer new knowledge becomes increasingly difficult and requires more and more specified applications. Fortunately, computation, storage and communication technologies steadily improve and enable the development of complex data processing application in both research and industry fields. Such an issue has gave birth to a new field of computer science called "Big Data" \cite{cite1}.\\

The term "Big Data" describes innovative techniques and technologies to capture, store, manage, distributes and analyze petabyte- or larger-sized datasets with high-velocity and different structures \cite{cite2}. Big Data is a data whose scale, diversity, and complexity require new architecture, techniques, algorithms, and analytics to manage it and extract value and hidden knowledge from it. Big data is a big deal, especially in such data-intensive industries as cybersecurity, finance, marketing, transportation, energy, and others. So, the key question is, "How do we extract knowledge from big data?"\\

The answer to this question is partly through analytics, which is also a growing field within various sectors. The world today is built on the foundations of data. Todays companies are forced to dispose, interrogate and manage data \cite{cite7}. The development of technology infrastructure is adapted to help process data, so that all the offered services can be improved as they are used.\\

In this dissertation, we identify three areas of research, which can provide significant improvements and benefits for fact discovery and acquisition of knowledge from large amount of data, particularly when combined and applied together:
\begin{itemize}
\item Architectures for parallel execution and sharing jobs are powerful tools to improve results throughout speed up execution time and avoid memory issues.
\item Advanced statistics algorithms can reduce complexity of unstructured data at large scale. It provides analytic overview and helps to extract knowledge from the data. 
\item Visual interfaces are a powerful means for exploring and analyzing large amount of data and providing access to knowledge and facts.\\ 
\end{itemize}

That's why we will consider an approach combining automatic methods and visual techniques to discover facts and derive new knowledge from massive data.\\
~\\ ~\\
This dissertation is structured around six chapters:\\

The first chapter, entitled \textbf{"Project Overview"}, includes a presentation of the organization in which we carried out our project. It also includes a definition of the system around which our project takes place.\\

The second chapter, entitled \textbf{"Preliminary Study"}, includes a theoretical review of the concepts on which our project is based on, a study of the existing, criticisms and a solution to our problem.\\

The third chapter, entitled \textbf{"Parallel Architectures"}, highlights the main features provided by our adopted architecture and shows how important is to choose such solution to perform execution time and maintain data security.\\

The forth chapter, entitled \textbf{"Requirements Analysis and Specification"}, describes the specification of the functional and technical requirements of the application.\\

The fifth chapter, entitled \textbf{"Design and Structure"}, clarifies the conceptual modelling of the application.\\

The final chapter, entitled \textbf{"Implementation and Results"}, includes a presentation of the working environment with a description of some interfaces of the application and execution result achievement.\\

Finally, we end up this dissertation with a general conclusion in which we summarize our solution and set forth some future perspectives.

